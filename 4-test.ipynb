{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 8723131,
     "sourceType": "datasetVersion",
     "datasetId": 4727572
    }
   ],
   "dockerImageVersionId": 30732,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# Setup environment\n",
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = '1'  # To fix ktrain installation\n",
    "os.environ['WANDB_API_KEY'] =  UserSecretsClient().get_secret(\"WANDB_API_KEY\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-25T13:06:20.249504Z",
     "iopub.execute_input": "2024-06-25T13:06:20.249859Z",
     "iopub.status.idle": "2024-06-25T13:06:20.446190Z",
     "shell.execute_reply.started": "2024-06-25T13:06:20.249829Z",
     "shell.execute_reply": "2024-06-25T13:06:20.445383Z"
    },
    "trusted": true
   },
   "id": "7b5c1cc52b4b1427",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Install dependencies\n!pip install ktrain wandb\n\n# Login to wandb\n!wandb login",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-25T13:06:20.448074Z",
     "iopub.execute_input": "2024-06-25T13:06:20.448425Z",
     "iopub.status.idle": "2024-06-25T13:07:07.826151Z",
     "shell.execute_reply.started": "2024-06-25T13:06:20.448393Z",
     "shell.execute_reply": "2024-06-25T13:07:07.824995Z"
    },
    "trusted": true
   },
   "id": "36f5f95e9ca9b7f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ktrain imports\n",
    "import ktrain\n",
    "\n",
    "# wandb import\n",
    "import wandb"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-25T13:07:07.827693Z",
     "iopub.execute_input": "2024-06-25T13:07:07.827991Z",
     "iopub.status.idle": "2024-06-25T13:07:25.726118Z",
     "shell.execute_reply.started": "2024-06-25T13:07:07.827964Z",
     "shell.execute_reply": "2024-06-25T13:07:25.725170Z"
    },
    "trusted": true
   },
   "id": "3d4624ce31082e60",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Hyperparameter values\n",
    "ALL_GENRES = ['Action', 'Adventure', 'Animation', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Family', 'Fantasy',\n",
    "              'History', 'Horror', 'Music', 'Mystery', 'Romance', 'Science Fiction', 'TV Movie', 'Thriller', 'War',\n",
    "              'Western']\n",
    "\n",
    "# GENRES\n",
    "GENRES_1 = ['Action', 'Adventure', 'Animation', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Family', 'Fantasy',\n",
    "            'History', 'Horror', 'Music', 'Mystery', 'Romance', 'Science Fiction', 'TV Movie', 'Thriller', 'War',\n",
    "            'Western']\n",
    "GENRES_2 = ['Comedy', 'Drama', 'Documentary', 'Romance', 'Horror', 'Action', 'Thriller', 'Family', 'Adventure',\n",
    "            'Crime', 'Science Fiction']"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-25T13:07:25.729366Z",
     "iopub.execute_input": "2024-06-25T13:07:25.729755Z",
     "iopub.status.idle": "2024-06-25T13:07:25.738308Z",
     "shell.execute_reply.started": "2024-06-25T13:07:25.729720Z",
     "shell.execute_reply": "2024-06-25T13:07:25.737110Z"
    },
    "trusted": true
   },
   "id": "eb51929b651a3629",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    " # HYPERPARAMETER CONSTANTS\n",
    "\n",
    "INCLUDED_GENRES = GENRES_2\n",
    "FILTERING_STRATEGY_FOR_GENRES = \"remove_only_labels\"  # remove_only_labels, remove_movies"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-25T13:07:25.739651Z",
     "iopub.execute_input": "2024-06-25T13:07:25.740022Z",
     "iopub.status.idle": "2024-06-25T13:07:25.751989Z",
     "shell.execute_reply.started": "2024-06-25T13:07:25.739989Z",
     "shell.execute_reply": "2024-06-25T13:07:25.751097Z"
    },
    "trusted": true
   },
   "id": "38589d8b5cc7abe9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# NON HYPERPARAMETER CONSTANTS\n\nTHRESHOLD = 0.5",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-25T13:07:25.753121Z",
     "iopub.execute_input": "2024-06-25T13:07:25.753523Z",
     "iopub.status.idle": "2024-06-25T13:07:25.762389Z",
     "shell.execute_reply.started": "2024-06-25T13:07:25.753498Z",
     "shell.execute_reply": "2024-06-25T13:07:25.761564Z"
    },
    "trusted": true
   },
   "id": "9a0122f09553a4c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Setup WANDB\nrun = wandb.init(\n    project='test-pms',\n    job_type='test',\n    save_code=True,\n    config={\n    },\n)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-25T13:07:25.763438Z",
     "iopub.execute_input": "2024-06-25T13:07:25.763708Z",
     "iopub.status.idle": "2024-06-25T13:07:45.206801Z",
     "shell.execute_reply.started": "2024-06-25T13:07:25.763684Z",
     "shell.execute_reply": "2024-06-25T13:07:45.205650Z"
    },
    "trusted": true
   },
   "id": "6134ff7c5fc7f6e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "- `remove_only_labels`: removes labels from samples that are not in the included genres and removes samples that have no labels left\n- `remove_movies`: removes samples where any of the labels are not in the included genres",
   "metadata": {},
   "id": "ef402f51f3c5dad2"
  },
  {
   "cell_type": "code",
   "source": "# Load data\ntest_path = '/kaggle/input/movie-genre/test.csv'\n\ntest_data = pd.read_csv(test_path)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-25T13:07:45.208467Z",
     "iopub.execute_input": "2024-06-25T13:07:45.208833Z",
     "iopub.status.idle": "2024-06-25T13:07:47.141154Z",
     "shell.execute_reply.started": "2024-06-25T13:07:45.208797Z",
     "shell.execute_reply": "2024-06-25T13:07:47.140114Z"
    },
    "trusted": true
   },
   "id": "4bd8c1cc26b9d0fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "feature_column = 'overview'\nlabel_columns = INCLUDED_GENRES",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-25T13:07:47.142480Z",
     "iopub.execute_input": "2024-06-25T13:07:47.142832Z",
     "iopub.status.idle": "2024-06-25T13:07:47.677191Z",
     "shell.execute_reply.started": "2024-06-25T13:07:47.142799Z",
     "shell.execute_reply": "2024-06-25T13:07:47.676176Z"
    },
    "trusted": true
   },
   "id": "f8a3e7e56d8e3a9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "from sklearn.model_selection import train_test_split\n\n\ndef split_data(_data, test_size=.2, random_state=42):\n    train, test = train_test_split(_data, test_size=test_size, random_state=random_state)\n\n    return train, test\n\n\ndef filter_data(_data, _label_columns, strategy):\n    if strategy == 'remove_only_labels':\n        label_sum = _data[_label_columns].sum(axis=1)\n        _data = _data[label_sum > 0]\n    elif strategy == 'remove_movies':\n        all_genres = set(ALL_GENRES)\n        not_interest_genres = all_genres - set(_label_columns)\n\n        for col in not_interest_genres:\n            _data = _data[data[col] == 0]\n    else:\n        raise ValueError(f\"Unknown strategy: {strategy}\")\n\n    return _data",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-25T13:07:47.681534Z",
     "iopub.execute_input": "2024-06-25T13:07:47.682135Z",
     "iopub.status.idle": "2024-06-25T13:07:48.309301Z",
     "shell.execute_reply.started": "2024-06-25T13:07:47.682109Z",
     "shell.execute_reply": "2024-06-25T13:07:48.308126Z"
    },
    "trusted": true
   },
   "id": "cea9f80b65bd43c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "filtering_strategy = FILTERING_STRATEGY_FOR_GENRES\n",
    "\n",
    "# Filter the data\n",
    "initial_number_of_samples = test_data.shape[0]\n",
    "data = filter_data(test_data, label_columns, filtering_strategy)\n",
    "\n",
    "data = data[label_columns + [col for col in data.columns if col not in label_columns]]\n",
    "\n",
    "# Print the number of excluded samples\n",
    "excluded_samples = initial_number_of_samples - data.shape[0]\n",
    "print(\"Number of excluded samples:\", excluded_samples)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-25T13:07:48.312746Z",
     "iopub.execute_input": "2024-06-25T13:07:48.313092Z",
     "iopub.status.idle": "2024-06-25T13:07:48.859314Z",
     "shell.execute_reply.started": "2024-06-25T13:07:48.313056Z",
     "shell.execute_reply": "2024-06-25T13:07:48.858345Z"
    },
    "trusted": true
   },
   "id": "45f5e6ad03a92053",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Prepare data format for keras use\nX_test = test_data[feature_column].tolist()\nY_test = test_data[label_columns].to_numpy()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-25T13:07:48.860610Z",
     "iopub.execute_input": "2024-06-25T13:07:48.860886Z",
     "iopub.status.idle": "2024-06-25T13:07:49.413198Z",
     "shell.execute_reply.started": "2024-06-25T13:07:48.860861Z",
     "shell.execute_reply": "2024-06-25T13:07:49.412274Z"
    },
    "trusted": true
   },
   "id": "3d5c06dd89a0f11e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Calculate class weights\n",
    "# Needed for weighted average evaluation, not for training\n",
    "\n",
    "genre_counts = np.sum(Y_test, axis=0)\n",
    "total_samples = len(Y_test)\n",
    "\n",
    "genre_freq = genre_counts / total_samples\n",
    "\n",
    "class_weights = {i: (1 / freq) if freq > 0 else 0 for i, freq in enumerate(genre_freq)}\n",
    "\n",
    "# Normalize class weights to make the minimum weight 1.0\n",
    "# https://www.analyticsvidhya.com/blog/2020/10/improve-class-imbalance-class-weights/\n",
    "min_weight = min(class_weights.values())\n",
    "class_weights = {i: weight / min_weight for i, weight in class_weights.items()}\n",
    "\n",
    "print(class_weights)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-25T13:07:49.414273Z",
     "iopub.execute_input": "2024-06-25T13:07:49.417462Z",
     "iopub.status.idle": "2024-06-25T13:07:49.902459Z",
     "shell.execute_reply.started": "2024-06-25T13:07:49.417436Z",
     "shell.execute_reply": "2024-06-25T13:07:49.901376Z"
    },
    "trusted": true
   },
   "id": "272ccf67d5588c07",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Load the model and the learner\n\n# Fetch artifact\nartifact = run.use_artifact(f'pms/model_epoch_4:latest')\n\n# Download the artifact\nartifact_dir = artifact.download()\n\n# Load the predictor\npredictor = ktrain.load_predictor(artifact_dir)\n\n# Load the preprocessor\npreprocessor = predictor.preproc",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-25T13:07:49.903586Z",
     "iopub.execute_input": "2024-06-25T13:07:49.903865Z",
     "iopub.status.idle": "2024-06-25T13:07:59.339584Z",
     "shell.execute_reply.started": "2024-06-25T13:07:49.903839Z",
     "shell.execute_reply": "2024-06-25T13:07:59.338686Z"
    },
    "trusted": true
   },
   "id": "cb724a0f48f17bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "val_data = preprocessor.preprocess_test(X_test, Y_test)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-25T13:07:59.340810Z",
     "iopub.execute_input": "2024-06-25T13:07:59.341149Z",
     "iopub.status.idle": "2024-06-25T13:08:03.057294Z",
     "shell.execute_reply.started": "2024-06-25T13:07:59.341118Z",
     "shell.execute_reply": "2024-06-25T13:08:03.056434Z"
    },
    "trusted": true
   },
   "id": "569e59ea656cb8ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Evaluation preparation",
   "metadata": {},
   "id": "eb87c727f119c0b0"
  },
  {
   "cell_type": "markdown",
   "source": "# FUNCTIONS FOR METRICS EVALUATION AND LOGGING",
   "metadata": {},
   "id": "8589efb3e15c823"
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Micro averaged metrics, or per instance metrics\n",
    "\n",
    "def exact_match_ratio(y_true, y_pred, epoch):\n",
    "    exact_match_ratio_score = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Epoch {epoch} - Exact Match Ratio: {exact_match_ratio_score}\")\n",
    "    run.summary[f\"exact_match_ratio_epoch_{epoch}\"] = exact_match_ratio_score\n",
    "\n",
    "\n",
    "def micro_accuracy(y_true, y_pred, epoch):\n",
    "    correctly_predicted_labels = np.logical_and(y_true, y_pred).sum(axis=1)  # Intersection of true and predicted labels\n",
    "    total_labels = np.logical_or(y_true, y_pred).sum(axis=1)  # Union of true and predicted labels   \n",
    "    instance_acc = correctly_predicted_labels / total_labels\n",
    "\n",
    "    average_accuracy = np.average(instance_acc)\n",
    "    print(f\"Epoch {epoch} - Overall Accuracy: {average_accuracy}\")\n",
    "    run.summary[f\"overall_accuracy_epoch_{epoch}\"] = average_accuracy\n",
    "\n",
    "\n",
    "def micro_precision(y_true, y_pred, epoch):\n",
    "    correctly_predicted_labels = np.logical_and(y_true, y_pred).sum(axis=1)  # | Y ∩ Z | in the formula \n",
    "    count_of_predicted_labels = y_pred.sum(axis=1)  # | Z | in the formula \n",
    "\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):  # Avoid division by zero\n",
    "        precision_per_instance = np.true_divide(correctly_predicted_labels, count_of_predicted_labels)\n",
    "        precision_per_instance[\n",
    "            count_of_predicted_labels == 0] = 0  # Set precision to 0 where there are no actual positives\n",
    "\n",
    "    average_precision = np.average(precision_per_instance)\n",
    "    print(f\"Epoch {epoch} - Overall Precision: {average_precision}\")\n",
    "    run.summary[f\"overall_precision_epoch_{epoch}\"] = average_precision\n",
    "\n",
    "\n",
    "def micro_recall(y_true, y_pred, epoch):\n",
    "    correctly_predicted_labels = np.logical_and(y_true, y_pred).sum(axis=1)  # | Y ∩ Z | in the formula\n",
    "    count_of_true_positives = y_true.sum(axis=1)  # | Y | in the formula\n",
    "\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):  # Avoid division by zero\n",
    "        recall_per_instance = np.true_divide(correctly_predicted_labels, count_of_true_positives)\n",
    "        recall_per_instance[count_of_true_positives == 0] = 0  # Set recall to 0 where there are no predicted positives\n",
    "\n",
    "    average_recall = np.average(recall_per_instance)\n",
    "    print(f\"Epoch {epoch} - Overall Recall: {average_recall}\")\n",
    "    run.summary[f\"overall_recall_epoch_{epoch}\"] = average_recall\n",
    "\n",
    "\n",
    "def micro_f1_score(y_true, y_pred, epoch):\n",
    "    true_positives = np.logical_and(y_true, y_pred).sum(axis=1)\n",
    "    total_actual_positives = y_true.sum(axis=1)\n",
    "    total_predicted_positives = y_pred.sum(axis=1)\n",
    "\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        f1_per_instance = 2 * true_positives / (total_actual_positives + total_predicted_positives)\n",
    "        f1_per_instance[np.isnan(f1_per_instance)] = 0  # Set F1 to 0 where there are no positives (actual or predicted)\n",
    "\n",
    "    average_f1 = np.average(f1_per_instance)\n",
    "\n",
    "    print(f\"Epoch {epoch} - Overall F1 Score: {average_f1}\")\n",
    "    run.summary[f\"overall_f1_score_epoch_{epoch}\"] = average_f1\n",
    "\n",
    "\n",
    "# Hamming loss shows error, lower is better\n",
    "def instance_hamming_loss(y_true, y_pred, epoch):\n",
    "    xor_result = np.logical_xor(y_true, y_pred)  # True if prediction is wrong\n",
    "\n",
    "    incorrect_predictions = np.sum(xor_result)  # Total number of incorrect predictions\n",
    "    print(f\"Total incorrect predictions: {incorrect_predictions}\")\n",
    "\n",
    "    total_labels = np.size(y_true)  # Total number of labels\n",
    "    print(f\"Total labels: {total_labels}\")\n",
    "    hamming_losses = incorrect_predictions / total_labels\n",
    "    average_hamming_loss = np.average(hamming_losses)\n",
    "\n",
    "    print(f\"Epoch {epoch} - Hamming Loss: {average_hamming_loss}\")\n",
    "    run.summary[f\"hamming_loss_epoch_{epoch}\"] = average_hamming_loss\n",
    "\n",
    "\n",
    "# Macro averaged metrics, or per label metrics\n",
    "\n",
    "def log_per_label_metric(metric_values, metric_name, genres, epoch):\n",
    "    for i, current_genre in enumerate(genres):\n",
    "        # TODO: maybe log this as a table\n",
    "        print(f\"Epoch {epoch} - Per Label {metric_name} {current_genre}: {metric_values[i]:.4f}\")\n",
    "        run.summary[f\"per_label_{metric_name}_{current_genre}_epoch_{epoch}\"] = metric_values[i]\n",
    "\n",
    "    log_per_label_metric_table(metric_values, metric_name, genres, epoch)\n",
    "\n",
    "\n",
    "def log_per_label_metric_table(metric_values, metric_name, genres, epoch):\n",
    "    table_rows = []\n",
    "\n",
    "    for i, current_genre in enumerate(genres):\n",
    "        row = [epoch, metric_name, current_genre, metric_values[i]]\n",
    "        table_rows.append(row)\n",
    "\n",
    "    table = wandb.Table(data=table_rows, columns=[\"Epoch\", \"Metric Name\", \"Genre\", \"Metric Value\"])\n",
    "\n",
    "    run.log({f\"per_label_{metric_name}_epoch_{epoch}\": table})\n",
    "\n",
    "\n",
    "def log_per_label_average(metric_values, metric_name, epoch):\n",
    "    average = np.average(metric_values)\n",
    "    print(f\"Per Label Average {metric_name} epoch {epoch}: {average:.4f}\")\n",
    "    run.summary[f\"per_label_average_{metric_name}_epoch_{epoch}\"] = average\n",
    "    run.summary[f\"per_label_average_{metric_name}\"] = average\n",
    "\n",
    "    log_per_label_weighted_average(metric_values, metric_name, epoch)\n",
    "\n",
    "\n",
    "def log_per_label_weighted_average(metric_values, metric_name, epoch):\n",
    "    weights = [class_weights[i] for i in range(len(metric_values))]\n",
    "    print(metric_values)\n",
    "    print(weights)\n",
    "    weighted_average = np.average(metric_values, weights=list(weights))\n",
    "    print(f\"Per Label Weighted Average {metric_name} epoich {epoch}: {weighted_average:.4f}\")\n",
    "    run.summary[f\"per_label_weighted_average_{metric_name}_epoch_{epoch}\"] = weighted_average\n",
    "    run.summary[f\"per_label_weighted_average_{metric_name}\"] = weighted_average\n",
    "\n",
    "\n",
    "def label_based_accuracy(y_true, y_pred, epoch):\n",
    "    num_labels = y_true.shape[1]\n",
    "    _accuracy_per_label = []\n",
    "\n",
    "    for label_idx in range(num_labels):\n",
    "        correct_predictions = np.sum(y_true[:, label_idx] == y_pred[:, label_idx])\n",
    "        total_predictions = y_true.shape[0]\n",
    "\n",
    "        label_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "        _accuracy_per_label.append(label_accuracy)\n",
    "\n",
    "    log_per_label_metric(_accuracy_per_label, 'Accuracy', INCLUDED_GENRES, epoch)\n",
    "    log_per_label_average(_accuracy_per_label, 'Accuracy', epoch)\n",
    "\n",
    "\n",
    "def precision_per_label(y_true, y_pred, epoch):\n",
    "    num_labels = y_true.shape[1]\n",
    "    _precision_per_label = []\n",
    "    for label_idx in range(num_labels):\n",
    "        true_positives = np.sum((y_pred[:, label_idx] == 1) & (y_true[:, label_idx] == 1))\n",
    "        false_positives = np.sum((y_pred[:, label_idx] == 1) & (y_true[:, label_idx] == 0))\n",
    "\n",
    "        label_precision = true_positives / (\n",
    "                true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "        _precision_per_label.append(label_precision)\n",
    "\n",
    "    log_per_label_metric(_precision_per_label, 'Precision', INCLUDED_GENRES, epoch)\n",
    "    log_per_label_average(_precision_per_label, 'Precision', epoch)\n",
    "\n",
    "\n",
    "def recall_per_label(y_true, y_pred, epoch):\n",
    "    num_labels = y_true.shape[1]\n",
    "    _recall_per_label = []\n",
    "    for label_idx in range(num_labels):\n",
    "        true_positives = np.sum((y_pred[:, label_idx] == 1) & (y_true[:, label_idx] == 1))\n",
    "        false_negatives = np.sum((y_pred[:, label_idx] == 0) & (y_true[:, label_idx] == 1))\n",
    "\n",
    "        label_recall = true_positives / (\n",
    "                true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "        _recall_per_label.append(label_recall)\n",
    "\n",
    "    log_per_label_metric(_recall_per_label, 'Recall', INCLUDED_GENRES, epoch)\n",
    "    log_per_label_average(_recall_per_label, 'Recall', epoch)\n",
    "\n",
    "\n",
    "def f1_score_per_label(y_true, y_pred, epoch):\n",
    "    num_labels = y_true.shape[1]\n",
    "    _f1_scores = []\n",
    "    for label_idx in range(num_labels):\n",
    "        true_positives = np.sum((y_pred[:, label_idx] == 1) & (y_true[:, label_idx] == 1))\n",
    "        false_positives = np.sum((y_pred[:, label_idx] == 1) & (y_true[:, label_idx] == 0))\n",
    "        false_negatives = np.sum((y_pred[:, label_idx] == 0) & (y_true[:, label_idx] == 1))\n",
    "\n",
    "        label_precision = true_positives / (\n",
    "                true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "        label_recall = true_positives / (\n",
    "                true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "\n",
    "        label_f1 = 2 * (label_precision * label_recall) / (label_precision + label_recall) if (\n",
    "                                                                                                      label_precision + label_recall) > 0 else 0\n",
    "        _f1_scores.append(label_f1)\n",
    "\n",
    "    log_per_label_metric(_f1_scores, 'F1', INCLUDED_GENRES, epoch)\n",
    "    log_per_label_average(_f1_scores, 'F1', epoch)\n",
    "\n",
    "\n",
    "# Calculate and log all metrics\n",
    "\n",
    "def calculate_and_log_metrics(true_values, predicted_values, _epoch):\n",
    "    # Exact match ratio\n",
    "    exact_match_ratio(true_values, predicted_values, _epoch)\n",
    "\n",
    "    # Micro averaged metrics\n",
    "    micro_accuracy(true_values, predicted_values, _epoch)\n",
    "    micro_precision(true_values, predicted_values, _epoch)\n",
    "    micro_recall(true_values, predicted_values, _epoch)\n",
    "    micro_f1_score(true_values, predicted_values, _epoch)\n",
    "    instance_hamming_loss(true_values, predicted_values, _epoch)\n",
    "\n",
    "    # Macro averaged metrics\n",
    "    label_based_accuracy(true_values, predicted_values, _epoch)\n",
    "    precision_per_label(true_values, predicted_values, _epoch)\n",
    "    recall_per_label(true_values, predicted_values, _epoch)\n",
    "    f1_score_per_label(true_values, predicted_values, _epoch)\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-25T13:08:03.058785Z",
     "iopub.execute_input": "2024-06-25T13:08:03.059165Z",
     "iopub.status.idle": "2024-06-25T13:08:03.725456Z",
     "shell.execute_reply.started": "2024-06-25T13:08:03.059133Z",
     "shell.execute_reply": "2024-06-25T13:08:03.724520Z"
    },
    "trusted": true
   },
   "id": "f12ab1fe57988bdf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Train and evaluate the model",
   "metadata": {},
   "id": "6aac17aeb3ef2dce"
  },
  {
   "cell_type": "code",
   "source": [
    "def predictions_to_probability_array(Y_pred):\n",
    "    all_ordered_probabilities = []\n",
    "\n",
    "    for prediction_set in Y_pred:\n",
    "        genre_to_prob = {genre_prob[0]: float(genre_prob[1]) for genre_prob in prediction_set}\n",
    "        ordered_probabilities = np.array([genre_to_prob[genre] for genre in INCLUDED_GENRES])\n",
    "        all_ordered_probabilities.append(ordered_probabilities)\n",
    "\n",
    "    all_ordered_probabilities = np.array(all_ordered_probabilities)\n",
    "\n",
    "    return all_ordered_probabilities"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-25T13:08:03.727055Z",
     "iopub.execute_input": "2024-06-25T13:08:03.727502Z",
     "iopub.status.idle": "2024-06-25T13:08:04.286644Z",
     "shell.execute_reply.started": "2024-06-25T13:08:03.727470Z",
     "shell.execute_reply": "2024-06-25T13:08:04.285739Z"
    },
    "trusted": true
   },
   "id": "f0392b9b2419192d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "total_training_time = 0\n",
    "\n",
    "# Predict the validation set\n",
    "Y_pred = predictor.predict(X_test)\n",
    "Y_pred = predictions_to_probability_array(Y_pred)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-25T13:08:04.288086Z",
     "iopub.execute_input": "2024-06-25T13:08:04.288457Z",
     "iopub.status.idle": "2024-06-25T13:09:03.697209Z",
     "shell.execute_reply.started": "2024-06-25T13:08:04.288401Z",
     "shell.execute_reply": "2024-06-25T13:09:03.696338Z"
    },
    "trusted": true
   },
   "id": "8637a036abc76c0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Binarize the predictions based on a threshold\nthreshold = THRESHOLD\nY_pred_binarized = (Y_pred > threshold).astype(int)\n\ncalculate_and_log_metrics(Y_test, Y_pred_binarized, 0)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-25T13:09:03.698439Z",
     "iopub.execute_input": "2024-06-25T13:09:03.698755Z",
     "iopub.status.idle": "2024-06-25T13:09:04.709606Z",
     "shell.execute_reply.started": "2024-06-25T13:09:03.698725Z",
     "shell.execute_reply": "2024-06-25T13:09:04.708518Z"
    },
    "trusted": true
   },
   "id": "cbb436c960705145",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "wandb.finish()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-25T13:13:56.996838Z",
     "iopub.execute_input": "2024-06-25T13:13:56.997821Z",
     "iopub.status.idle": "2024-06-25T13:14:04.471528Z",
     "shell.execute_reply.started": "2024-06-25T13:13:56.997778Z",
     "shell.execute_reply": "2024-06-25T13:14:04.470811Z"
    },
    "trusted": true
   },
   "id": "c5a424d8c9f3fc0f",
   "outputs": [],
   "execution_count": null
  }
 ]
}
